{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama_index==0.10.19\n",
      "  Using cached llama_index-0.10.19-py3-none-any.whl.metadata (8.8 kB)\n",
      "Collecting llama_index_core==0.10.19\n",
      "  Using cached llama_index_core-0.10.19-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting torch\n",
      "  Using cached torch-2.5.1-cp310-cp310-win_amd64.whl.metadata (28 kB)\n",
      "Collecting llama-index-embeddings-huggingface\n",
      "  Using cached llama_index_embeddings_huggingface-0.4.0-py3-none-any.whl.metadata (767 bytes)\n",
      "Collecting peft\n",
      "  Using cached peft-0.14.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting optimum\n",
      "  Using cached optimum-1.23.3-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting bitsandbytes\n",
      "  Using cached bitsandbytes-0.45.0-py3-none-win_amd64.whl.metadata (2.9 kB)\n",
      "Collecting llama-index-agent-openai<0.2.0,>=0.1.4 (from llama_index==0.10.19)\n",
      "  Using cached llama_index_agent_openai-0.1.7-py3-none-any.whl.metadata (644 bytes)\n",
      "Collecting llama-index-cli<0.2.0,>=0.1.2 (from llama_index==0.10.19)\n",
      "  Using cached llama_index_cli-0.1.13-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting llama-index-embeddings-openai<0.2.0,>=0.1.5 (from llama_index==0.10.19)\n",
      "  Using cached llama_index_embeddings_openai-0.1.11-py3-none-any.whl.metadata (655 bytes)\n",
      "Collecting llama-index-indices-managed-llama-cloud<0.2.0,>=0.1.2 (from llama_index==0.10.19)\n",
      "  Using cached llama_index_indices_managed_llama_cloud-0.1.6-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama_index==0.10.19)\n",
      "  Using cached llama_index_legacy-0.9.48.post4-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting llama-index-llms-openai<0.2.0,>=0.1.5 (from llama_index==0.10.19)\n",
      "  Using cached llama_index_llms_openai-0.1.31-py3-none-any.whl.metadata (650 bytes)\n",
      "Collecting llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 (from llama_index==0.10.19)\n",
      "  Using cached llama_index_multi_modal_llms_openai-0.1.9-py3-none-any.whl.metadata (728 bytes)\n",
      "Collecting llama-index-program-openai<0.2.0,>=0.1.3 (from llama_index==0.10.19)\n",
      "  Using cached llama_index_program_openai-0.1.7-py3-none-any.whl.metadata (760 bytes)\n",
      "Collecting llama-index-question-gen-openai<0.2.0,>=0.1.2 (from llama_index==0.10.19)\n",
      "  Using cached llama_index_question_gen_openai-0.1.3-py3-none-any.whl.metadata (785 bytes)\n",
      "Collecting llama-index-readers-file<0.2.0,>=0.1.4 (from llama_index==0.10.19)\n",
      "  Using cached llama_index_readers_file-0.1.33-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting llama-index-readers-llama-parse<0.2.0,>=0.1.2 (from llama_index==0.10.19)\n",
      "  Using cached llama_index_readers_llama_parse-0.1.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting PyYAML>=6.0.1 (from llama_index_core==0.10.19)\n",
      "  Using cached PyYAML-6.0.2-cp310-cp310-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting SQLAlchemy>=1.4.49 (from SQLAlchemy[asyncio]>=1.4.49->llama_index_core==0.10.19)\n",
      "  Using cached SQLAlchemy-2.0.36-cp310-cp310-win_amd64.whl.metadata (9.9 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.6 (from llama_index_core==0.10.19)\n",
      "  Downloading aiohttp-3.11.10-cp310-cp310-win_amd64.whl.metadata (8.0 kB)\n",
      "Collecting dataclasses-json (from llama_index_core==0.10.19)\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting deprecated>=1.2.9.3 (from llama_index_core==0.10.19)\n",
      "  Using cached Deprecated-1.2.15-py2.py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama_index_core==0.10.19)\n",
      "  Using cached dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting fsspec>=2023.5.0 (from llama_index_core==0.10.19)\n",
      "  Using cached fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting httpx (from llama_index_core==0.10.19)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting llamaindex-py-client<0.2.0,>=0.1.13 (from llama_index_core==0.10.19)\n",
      "  Using cached llamaindex_py_client-0.1.19-py3-none-any.whl.metadata (760 bytes)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\users\\ajay2\\onedrive\\desktop\\guvi\\project_cap\\llama rag\\.venv\\lib\\site-packages (from llama_index_core==0.10.19) (1.6.0)\n",
      "Collecting networkx>=3.0 (from llama_index_core==0.10.19)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting nltk<4.0.0,>=3.8.1 (from llama_index_core==0.10.19)\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting numpy (from llama_index_core==0.10.19)\n",
      "  Using cached numpy-2.2.0-cp310-cp310-win_amd64.whl.metadata (60 kB)\n",
      "Collecting openai>=1.1.0 (from llama_index_core==0.10.19)\n",
      "  Downloading openai-1.58.1-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting pandas (from llama_index_core==0.10.19)\n",
      "  Using cached pandas-2.2.3-cp310-cp310-win_amd64.whl.metadata (19 kB)\n",
      "Collecting pillow>=9.0.0 (from llama_index_core==0.10.19)\n",
      "  Using cached pillow-11.0.0-cp310-cp310-win_amd64.whl.metadata (9.3 kB)\n",
      "Collecting requests>=2.31.0 (from llama_index_core==0.10.19)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tenacity<9.0.0,>=8.2.0 (from llama_index_core==0.10.19)\n",
      "  Using cached tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting tiktoken>=0.3.3 (from llama_index_core==0.10.19)\n",
      "  Using cached tiktoken-0.8.0-cp310-cp310-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting tqdm<5.0.0,>=4.66.1 (from llama_index_core==0.10.19)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\ajay2\\onedrive\\desktop\\guvi\\project_cap\\llama rag\\.venv\\lib\\site-packages (from llama_index_core==0.10.19) (4.12.2)\n",
      "Collecting typing-inspect>=0.8.0 (from llama_index_core==0.10.19)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Using cached filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting huggingface-hub>=0.19.0 (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface)\n",
      "  Downloading huggingface_hub-0.27.0-py3-none-any.whl.metadata (13 kB)\n",
      "INFO: pip is looking at multiple versions of llama-index-embeddings-huggingface to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting llama-index-embeddings-huggingface\n",
      "  Using cached llama_index_embeddings_huggingface-0.3.1-py3-none-any.whl.metadata (718 bytes)\n",
      "  Using cached llama_index_embeddings_huggingface-0.3.0-py3-none-any.whl.metadata (769 bytes)\n",
      "  Using cached llama_index_embeddings_huggingface-0.2.3-py3-none-any.whl.metadata (769 bytes)\n",
      "Collecting sentence-transformers>=2.6.1 (from llama-index-embeddings-huggingface)\n",
      "  Using cached sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ajay2\\onedrive\\desktop\\guvi\\project_cap\\llama rag\\.venv\\lib\\site-packages (from peft) (24.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\ajay2\\onedrive\\desktop\\guvi\\project_cap\\llama rag\\.venv\\lib\\site-packages (from peft) (6.1.0)\n",
      "Collecting transformers (from peft)\n",
      "  Downloading transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting accelerate>=0.21.0 (from peft)\n",
      "  Using cached accelerate-1.2.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting safetensors (from peft)\n",
      "  Using cached safetensors-0.4.5-cp310-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting coloredlogs (from optimum)\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting datasets (from optimum)\n",
      "  Using cached datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0,>=3.8.6->llama_index_core==0.10.19)\n",
      "  Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.6->llama_index_core==0.10.19)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp<4.0.0,>=3.8.6->llama_index_core==0.10.19)\n",
      "  Using cached async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.6->llama_index_core==0.10.19)\n",
      "  Downloading attrs-24.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.6->llama_index_core==0.10.19)\n",
      "  Using cached frozenlist-1.5.0-cp310-cp310-win_amd64.whl.metadata (14 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.6->llama_index_core==0.10.19)\n",
      "  Using cached multidict-6.1.0-cp310-cp310-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.6->llama_index_core==0.10.19)\n",
      "  Downloading propcache-0.2.1-cp310-cp310-win_amd64.whl.metadata (9.5 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.6->llama_index_core==0.10.19)\n",
      "  Downloading yarl-1.18.3-cp310-cp310-win_amd64.whl.metadata (71 kB)\n",
      "Collecting wrapt<2,>=1.10 (from deprecated>=1.2.9.3->llama_index_core==0.10.19)\n",
      "  Using cached wrapt-1.17.0-cp310-cp310-win_amd64.whl.metadata (6.5 kB)\n",
      "INFO: pip is looking at multiple versions of llama-index-llms-openai to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting llama-index-llms-openai<0.2.0,>=0.1.5 (from llama_index==0.10.19)\n",
      "  Using cached llama_index_llms_openai-0.1.30-py3-none-any.whl.metadata (650 bytes)\n",
      "  Using cached llama_index_llms_openai-0.1.29-py3-none-any.whl.metadata (650 bytes)\n",
      "  Using cached llama_index_llms_openai-0.1.28-py3-none-any.whl.metadata (650 bytes)\n",
      "  Using cached llama_index_llms_openai-0.1.27-py3-none-any.whl.metadata (610 bytes)\n",
      "  Using cached llama_index_llms_openai-0.1.26-py3-none-any.whl.metadata (610 bytes)\n",
      "  Using cached llama_index_llms_openai-0.1.25-py3-none-any.whl.metadata (610 bytes)\n",
      "  Using cached llama_index_llms_openai-0.1.24-py3-none-any.whl.metadata (610 bytes)\n",
      "INFO: pip is still looking at multiple versions of llama-index-llms-openai to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached llama_index_llms_openai-0.1.23-py3-none-any.whl.metadata (610 bytes)\n",
      "  Using cached llama_index_llms_openai-0.1.22-py3-none-any.whl.metadata (559 bytes)\n",
      "  Using cached llama_index_llms_openai-0.1.21-py3-none-any.whl.metadata (559 bytes)\n",
      "  Using cached llama_index_llms_openai-0.1.20-py3-none-any.whl.metadata (559 bytes)\n",
      "  Using cached llama_index_llms_openai-0.1.19-py3-none-any.whl.metadata (559 bytes)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Using cached llama_index_llms_openai-0.1.18-py3-none-any.whl.metadata (559 bytes)\n",
      "  Using cached llama_index_llms_openai-0.1.17-py3-none-any.whl.metadata (559 bytes)\n",
      "  Using cached llama_index_llms_openai-0.1.16-py3-none-any.whl.metadata (559 bytes)\n",
      "  Using cached llama_index_llms_openai-0.1.15-py3-none-any.whl.metadata (559 bytes)\n",
      "  Using cached llama_index_llms_openai-0.1.14-py3-none-any.whl.metadata (559 bytes)\n",
      "  Using cached llama_index_llms_openai-0.1.13-py3-none-any.whl.metadata (559 bytes)\n",
      "  Using cached llama_index_llms_openai-0.1.12-py3-none-any.whl.metadata (565 bytes)\n",
      "  Using cached llama_index_llms_openai-0.1.11-py3-none-any.whl.metadata (559 bytes)\n",
      "INFO: pip is looking at multiple versions of llama-index-program-openai to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting llama-index-program-openai<0.2.0,>=0.1.3 (from llama_index==0.10.19)\n",
      "  Using cached llama_index_program_openai-0.1.6-py3-none-any.whl.metadata (715 bytes)\n",
      "Collecting beautifulsoup4<5.0.0,>=4.12.3 (from llama-index-readers-file<0.2.0,>=0.1.4->llama_index==0.10.19)\n",
      "  Using cached beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "INFO: pip is looking at multiple versions of llama-index-readers-file to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting llama-index-readers-file<0.2.0,>=0.1.4 (from llama_index==0.10.19)\n",
      "  Using cached llama_index_readers_file-0.1.32-py3-none-any.whl.metadata (5.4 kB)\n",
      "  Using cached llama_index_readers_file-0.1.31-py3-none-any.whl.metadata (5.4 kB)\n",
      "  Using cached llama_index_readers_file-0.1.30-py3-none-any.whl.metadata (5.4 kB)\n",
      "  Using cached llama_index_readers_file-0.1.29-py3-none-any.whl.metadata (5.4 kB)\n",
      "  Using cached llama_index_readers_file-0.1.28-py3-none-any.whl.metadata (5.4 kB)\n",
      "  Using cached llama_index_readers_file-0.1.27-py3-none-any.whl.metadata (5.4 kB)\n",
      "  Using cached llama_index_readers_file-0.1.26-py3-none-any.whl.metadata (5.4 kB)\n",
      "INFO: pip is still looking at multiple versions of llama-index-readers-file to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached llama_index_readers_file-0.1.25-py3-none-any.whl.metadata (5.4 kB)\n",
      "  Using cached llama_index_readers_file-0.1.23-py3-none-any.whl.metadata (5.4 kB)\n",
      "  Using cached llama_index_readers_file-0.1.22-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting pypdf<5.0.0,>=4.0.1 (from llama-index-readers-file<0.2.0,>=0.1.4->llama_index==0.10.19)\n",
      "  Using cached pypdf-4.3.1-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.2.0,>=0.1.4->llama_index==0.10.19)\n",
      "  Using cached striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama_index==0.10.19)\n",
      "  Using cached llama_parse-0.5.17-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting pydantic>=1.10 (from llamaindex-py-client<0.2.0,>=0.1.13->llama_index_core==0.10.19)\n",
      "  Downloading pydantic-2.10.3-py3-none-any.whl.metadata (172 kB)\n",
      "Collecting anyio (from httpx->llama_index_core==0.10.19)\n",
      "  Downloading anyio-4.7.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting certifi (from httpx->llama_index_core==0.10.19)\n",
      "  Downloading certifi-2024.12.14-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting httpcore==1.* (from httpx->llama_index_core==0.10.19)\n",
      "  Using cached httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting idna (from httpx->llama_index_core==0.10.19)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->llama_index_core==0.10.19)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting click (from nltk<4.0.0,>=3.8.1->llama_index_core==0.10.19)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting joblib (from nltk<4.0.0,>=3.8.1->llama_index_core==0.10.19)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk<4.0.0,>=3.8.1->llama_index_core==0.10.19)\n",
      "  Using cached regex-2024.11.6-cp310-cp310-win_amd64.whl.metadata (41 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai>=1.1.0->llama_index_core==0.10.19)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai>=1.1.0->llama_index_core==0.10.19)\n",
      "  Downloading jiter-0.8.2-cp310-cp310-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting sniffio (from openai>=1.1.0->llama_index_core==0.10.19)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.31.0->llama_index_core==0.10.19)\n",
      "  Using cached charset_normalizer-3.4.0-cp310-cp310-win_amd64.whl.metadata (34 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.31.0->llama_index_core==0.10.19)\n",
      "  Using cached urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting scikit-learn (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Using cached scikit_learn-1.6.0-cp310-cp310-win_amd64.whl.metadata (15 kB)\n",
      "Collecting scipy (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Using cached scipy-1.14.1-cp310-cp310-win_amd64.whl.metadata (60 kB)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama_index_core==0.10.19)\n",
      "  Using cached greenlet-3.1.1-cp310-cp310-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\ajay2\\onedrive\\desktop\\guvi\\project_cap\\llama rag\\.venv\\lib\\site-packages (from tqdm<5.0.0,>=4.66.1->llama_index_core==0.10.19) (0.4.6)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers->peft)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama_index_core==0.10.19)\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->optimum)\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama_index_core==0.10.19)\n",
      "  Using cached marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting pyarrow>=15.0.0 (from datasets->optimum)\n",
      "  Using cached pyarrow-18.1.0-cp310-cp310-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets->optimum)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting xxhash (from datasets->optimum)\n",
      "  Using cached xxhash-3.5.0-cp310-cp310-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets->optimum)\n",
      "  Using cached multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec>=2023.5.0 (from llama_index_core==0.10.19)\n",
      "  Using cached fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Using cached MarkupSafe-3.0.2-cp310-cp310-win_amd64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ajay2\\onedrive\\desktop\\guvi\\project_cap\\llama rag\\.venv\\lib\\site-packages (from pandas->llama_index_core==0.10.19) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->llama_index_core==0.10.19)\n",
      "  Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->llama_index_core==0.10.19)\n",
      "  Using cached tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\ajay2\\onedrive\\desktop\\guvi\\project_cap\\llama rag\\.venv\\lib\\site-packages (from anyio->httpx->llama_index_core==0.10.19) (1.2.2)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama_index==0.10.19)\n",
      "  Using cached soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting pyreadline3 (from humanfriendly>=9.1->coloredlogs->optimum)\n",
      "  Using cached pyreadline3-3.5.4-py3-none-any.whl.metadata (4.7 kB)\n",
      "INFO: pip is looking at multiple versions of llama-parse to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama_index==0.10.19)\n",
      "  Using cached llama_parse-0.5.16-py3-none-any.whl.metadata (7.0 kB)\n",
      "  Using cached llama_parse-0.5.15-py3-none-any.whl.metadata (7.0 kB)\n",
      "  Using cached llama_parse-0.5.14-py3-none-any.whl.metadata (6.9 kB)\n",
      "  Using cached llama_parse-0.5.13-py3-none-any.whl.metadata (6.9 kB)\n",
      "  Using cached llama_parse-0.5.12-py3-none-any.whl.metadata (6.9 kB)\n",
      "  Using cached llama_parse-0.5.11-py3-none-any.whl.metadata (6.9 kB)\n",
      "  Using cached llama_parse-0.5.10-py3-none-any.whl.metadata (6.9 kB)\n",
      "INFO: pip is still looking at multiple versions of llama-parse to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached llama_parse-0.5.9-py3-none-any.whl.metadata (6.9 kB)\n",
      "  Using cached llama_parse-0.5.8-py3-none-any.whl.metadata (6.4 kB)\n",
      "  Using cached llama_parse-0.5.7-py3-none-any.whl.metadata (6.4 kB)\n",
      "  Using cached llama_parse-0.5.6-py3-none-any.whl.metadata (6.1 kB)\n",
      "  Using cached llama_parse-0.5.5-py3-none-any.whl.metadata (6.1 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Using cached llama_parse-0.5.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "  Using cached llama_parse-0.5.3-py3-none-any.whl.metadata (4.5 kB)\n",
      "  Using cached llama_parse-0.5.2-py3-none-any.whl.metadata (4.5 kB)\n",
      "  Using cached llama_parse-0.5.1-py3-none-any.whl.metadata (4.5 kB)\n",
      "  Using cached llama_parse-0.5.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "  Using cached llama_parse-0.4.9-py3-none-any.whl.metadata (4.4 kB)\n",
      "  Using cached llama_parse-0.4.8-py3-none-any.whl.metadata (4.4 kB)\n",
      "  Using cached llama_parse-0.4.7-py3-none-any.whl.metadata (4.4 kB)\n",
      "  Using cached llama_parse-0.4.6-py3-none-any.whl.metadata (4.4 kB)\n",
      "  Using cached llama_parse-0.4.5-py3-none-any.whl.metadata (4.4 kB)\n",
      "  Using cached llama_parse-0.4.4-py3-none-any.whl.metadata (3.5 kB)\n",
      "  Using cached llama_parse-0.4.3-py3-none-any.whl.metadata (3.5 kB)\n",
      "  Using cached llama_parse-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "  Using cached llama_parse-0.4.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "  Using cached llama_parse-0.4.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.13->llama_index_core==0.10.19)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.1 (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.13->llama_index_core==0.10.19)\n",
      "  Downloading pydantic_core-2.27.1-cp310-none-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ajay2\\onedrive\\desktop\\guvi\\project_cap\\llama rag\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->llama_index_core==0.10.19) (1.17.0)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Using cached llama_index-0.10.19-py3-none-any.whl (5.6 kB)\n",
      "Using cached llama_index_core-0.10.19-py3-none-any.whl (15.3 MB)\n",
      "Using cached torch-2.5.1-cp310-cp310-win_amd64.whl (203.1 MB)\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Using cached llama_index_embeddings_huggingface-0.2.3-py3-none-any.whl (8.6 kB)\n",
      "Using cached peft-0.14.0-py3-none-any.whl (374 kB)\n",
      "Using cached optimum-1.23.3-py3-none-any.whl (424 kB)\n",
      "Using cached bitsandbytes-0.45.0-py3-none-win_amd64.whl (68.5 MB)\n",
      "Using cached accelerate-1.2.1-py3-none-any.whl (336 kB)\n",
      "Downloading aiohttp-3.11.10-cp310-cp310-win_amd64.whl (441 kB)\n",
      "Using cached Deprecated-1.2.15-py2.py3-none-any.whl (9.9 kB)\n",
      "Using cached dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Downloading huggingface_hub-0.27.0-py3-none-any.whl (450 kB)\n",
      "Using cached llama_index_agent_openai-0.1.7-py3-none-any.whl (12 kB)\n",
      "Using cached llama_index_cli-0.1.13-py3-none-any.whl (27 kB)\n",
      "Using cached llama_index_embeddings_openai-0.1.11-py3-none-any.whl (6.3 kB)\n",
      "Using cached llama_index_indices_managed_llama_cloud-0.1.6-py3-none-any.whl (6.7 kB)\n",
      "Using cached llama_index_legacy-0.9.48.post4-py3-none-any.whl (1.2 MB)\n",
      "Using cached llama_index_llms_openai-0.1.11-py3-none-any.whl (10.0 kB)\n",
      "Using cached llama_index_multi_modal_llms_openai-0.1.9-py3-none-any.whl (5.9 kB)\n",
      "Using cached llama_index_program_openai-0.1.6-py3-none-any.whl (5.2 kB)\n",
      "Using cached llama_index_question_gen_openai-0.1.3-py3-none-any.whl (2.9 kB)\n",
      "Using cached llama_index_readers_file-0.1.22-py3-none-any.whl (36 kB)\n",
      "Using cached llama_index_readers_llama_parse-0.1.6-py3-none-any.whl (2.5 kB)\n",
      "Using cached llamaindex_py_client-0.1.19-py3-none-any.whl (141 kB)\n",
      "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached numpy-2.2.0-cp310-cp310-win_amd64.whl (12.9 MB)\n",
      "Downloading openai-1.58.1-py3-none-any.whl (454 kB)\n",
      "Using cached pillow-11.0.0-cp310-cp310-win_amd64.whl (2.6 MB)\n",
      "Using cached PyYAML-6.0.2-cp310-cp310-win_amd64.whl (161 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached safetensors-0.4.5-cp310-none-win_amd64.whl (285 kB)\n",
      "Using cached sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n",
      "Using cached SQLAlchemy-2.0.36-cp310-cp310-win_amd64.whl (2.1 MB)\n",
      "Using cached tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Using cached tiktoken-0.8.0-cp310-cp310-win_amd64.whl (884 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading transformers-4.47.1-py3-none-any.whl (10.1 MB)\n",
      "   ---------------------------------------- 0.0/10.1 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.5/10.1 MB 8.5 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.3/10.1 MB 2.9 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 2.4/10.1 MB 4.2 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 2.6/10.1 MB 3.9 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 3.7/10.1 MB 3.4 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 5.0/10.1 MB 4.0 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 5.0/10.1 MB 4.0 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 6.3/10.1 MB 3.6 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 7.6/10.1 MB 4.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 7.6/10.1 MB 4.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.4/10.1 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.0/10.1 MB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.1/10.1 MB 3.8 MB/s eta 0:00:00\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Using cached datasets-3.2.0-py3-none-any.whl (480 kB)\n",
      "Using cached fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "Using cached filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Using cached pandas-2.2.3-cp310-cp310-win_amd64.whl (11.6 MB)\n",
      "Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading anyio-4.7.0-py3-none-any.whl (93 kB)\n",
      "Using cached async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading attrs-24.3.0-py3-none-any.whl (63 kB)\n",
      "Using cached beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "Downloading certifi-2024.12.14-py3-none-any.whl (164 kB)\n",
      "Using cached charset_normalizer-3.4.0-cp310-cp310-win_amd64.whl (102 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached frozenlist-1.5.0-cp310-cp310-win_amd64.whl (51 kB)\n",
      "Using cached greenlet-3.1.1-cp310-cp310-win_amd64.whl (298 kB)\n",
      "Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading jiter-0.8.2-cp310-cp310-win_amd64.whl (204 kB)\n",
      "Using cached llama_parse-0.4.0-py3-none-any.whl (7.0 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp310-cp310-win_amd64.whl (15 kB)\n",
      "Using cached marshmallow-3.23.1-py3-none-any.whl (49 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached multidict-6.1.0-cp310-cp310-win_amd64.whl (28 kB)\n",
      "Using cached multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Downloading propcache-0.2.1-cp310-cp310-win_amd64.whl (44 kB)\n",
      "Using cached pyarrow-18.1.0-cp310-cp310-win_amd64.whl (25.1 MB)\n",
      "Downloading pydantic-2.10.3-py3-none-any.whl (456 kB)\n",
      "Downloading pydantic_core-2.27.1-cp310-none-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ------------------------------- -------- 1.6/2.0 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 4.6 MB/s eta 0:00:00\n",
      "Using cached pypdf-4.3.1-py3-none-any.whl (295 kB)\n",
      "Using cached pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Using cached regex-2024.11.6-cp310-cp310-win_amd64.whl (274 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ------------------------------ --------- 1.8/2.4 MB 10.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.4/2.4 MB 8.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 4.7 MB/s eta 0:00:00\n",
      "Using cached tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Using cached urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Using cached wrapt-1.17.0-cp310-cp310-win_amd64.whl (38 kB)\n",
      "Downloading yarl-1.18.3-cp310-cp310-win_amd64.whl (90 kB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached scikit_learn-1.6.0-cp310-cp310-win_amd64.whl (11.1 MB)\n",
      "Using cached scipy-1.14.1-cp310-cp310-win_amd64.whl (44.8 MB)\n",
      "Using cached xxhash-3.5.0-cp310-cp310-win_amd64.whl (30 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Using cached soupsieve-2.6-py3-none-any.whl (36 kB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Using cached pyreadline3-3.5.4-py3-none-any.whl (83 kB)\n",
      "Installing collected packages: striprtf, pytz, mpmath, dirtyjson, xxhash, wrapt, urllib3, tzdata, tqdm, threadpoolctl, tenacity, sympy, soupsieve, sniffio, safetensors, regex, PyYAML, pyreadline3, pypdf, pydantic-core, pyarrow, propcache, pillow, numpy, networkx, mypy-extensions, multidict, marshmallow, MarkupSafe, joblib, jiter, idna, h11, greenlet, fsspec, frozenlist, filelock, distro, dill, click, charset-normalizer, certifi, attrs, async-timeout, annotated-types, aiohappyeyeballs, yarl, typing-inspect, SQLAlchemy, scipy, requests, pydantic, pandas, nltk, multiprocess, jinja2, humanfriendly, httpcore, deprecated, beautifulsoup4, anyio, aiosignal, torch, tiktoken, scikit-learn, huggingface-hub, httpx, dataclasses-json, coloredlogs, aiohttp, tokenizers, openai, llamaindex-py-client, bitsandbytes, accelerate, transformers, llama-index-legacy, llama_index_core, datasets, sentence-transformers, peft, optimum, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-embeddings-huggingface, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama_index\n",
      "Successfully installed MarkupSafe-3.0.2 PyYAML-6.0.2 SQLAlchemy-2.0.36 accelerate-1.2.1 aiohappyeyeballs-2.4.4 aiohttp-3.11.10 aiosignal-1.3.2 annotated-types-0.7.0 anyio-4.7.0 async-timeout-5.0.1 attrs-24.3.0 beautifulsoup4-4.12.3 bitsandbytes-0.45.0 certifi-2024.12.14 charset-normalizer-3.4.0 click-8.1.7 coloredlogs-15.0.1 dataclasses-json-0.6.7 datasets-3.2.0 deprecated-1.2.15 dill-0.3.8 dirtyjson-1.0.8 distro-1.9.0 filelock-3.16.1 frozenlist-1.5.0 fsspec-2024.9.0 greenlet-3.1.1 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 huggingface-hub-0.27.0 humanfriendly-10.0 idna-3.10 jinja2-3.1.4 jiter-0.8.2 joblib-1.4.2 llama-index-agent-openai-0.1.7 llama-index-cli-0.1.13 llama-index-embeddings-huggingface-0.2.3 llama-index-embeddings-openai-0.1.11 llama-index-indices-managed-llama-cloud-0.1.6 llama-index-legacy-0.9.48.post4 llama-index-llms-openai-0.1.11 llama-index-multi-modal-llms-openai-0.1.9 llama-index-program-openai-0.1.6 llama-index-question-gen-openai-0.1.3 llama-index-readers-file-0.1.22 llama-index-readers-llama-parse-0.1.6 llama-parse-0.4.0 llama_index-0.10.19 llama_index_core-0.10.19 llamaindex-py-client-0.1.19 marshmallow-3.23.1 mpmath-1.3.0 multidict-6.1.0 multiprocess-0.70.16 mypy-extensions-1.0.0 networkx-3.4.2 nltk-3.9.1 numpy-2.2.0 openai-1.58.1 optimum-1.23.3 pandas-2.2.3 peft-0.14.0 pillow-11.0.0 propcache-0.2.1 pyarrow-18.1.0 pydantic-2.10.3 pydantic-core-2.27.1 pypdf-4.3.1 pyreadline3-3.5.4 pytz-2024.2 regex-2024.11.6 requests-2.32.3 safetensors-0.4.5 scikit-learn-1.6.0 scipy-1.14.1 sentence-transformers-3.3.1 sniffio-1.3.1 soupsieve-2.6 striprtf-0.0.26 sympy-1.13.1 tenacity-8.5.0 threadpoolctl-3.5.0 tiktoken-0.8.0 tokenizers-0.21.0 torch-2.5.1 tqdm-4.67.1 transformers-4.47.1 typing-inspect-0.9.0 tzdata-2024.2 urllib3-2.2.3 wrapt-1.17.0 xxhash-3.5.0 yarl-1.18.3\n"
     ]
    }
   ],
   "source": [
    "! pip install llama_index==0.10.19 llama_index_core==0.10.19 torch llama-index-embeddings-huggingface peft optimum bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ajay2\\OneDrive\\Desktop\\guvi\\PROJECT_CAP\\LLAMA RAG\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings,SimpleDirectoryReader,VectorStoreIndex\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "from transformers import AutoModelForCausalLM,AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To globally set the resources we are goin to use\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    }
   ],
   "source": [
    "Settings.embed_model=HuggingFaceEmbedding(model_name='BAAI/bge-small-en-v1.5')\n",
    "Settings.llm = None\n",
    "Settings.chunk_size = 256 \n",
    "Settings.chunk_overlap = 15 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n",
      "48\n"
     ]
    }
   ],
   "source": [
    "documents=SimpleDirectoryReader('content').load_data()\n",
    "\n",
    "print(len(documents))\n",
    "\n",
    "\n",
    "for doc in documents:\n",
    "    if len(doc.text)== 0:\n",
    "        documents.remove(doc)\n",
    "        continue\n",
    "\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "index= VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "# Set no of docs to retrive\n",
    "top_k=2\n",
    "\n",
    "# Configure retriver\n",
    "retriever=VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=top_k,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assembling the Query engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine=RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.5)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context information is below.\n",
      "---------------------\n",
      "page_label: 33\n",
      "file_path: c:\\Users\\ajay2\\OneDrive\\Desktop\\guvi\\PROJECT_CAP\\LLAMA RAG\\content\\2308.12950v3.pdf\n",
      "\n",
      "0 0.0 0.0\n",
      "freq. scaling 1/4 100.0 100.0 100.0 100.0 99.2 99.2 2.34 99.2 100.0 0.0 0.0 0.0\n",
      "Ours (θ= 106) 95.3 95.3 100.0 100.0 95.3 100.0 54.7 100.0 98.4 3.1 85.9 85.9\n",
      "Table 18: Function Key Retrieval Accuracy (%) Ablations . Ablation experiments are performed with\n",
      "an earlier version of the 7B model; the last row refers to Code Llama 7B. All long context fine-tuning runs\n",
      "employ a sequence length of 16,384 tokens.\n",
      "\n",
      "page_label: 48\n",
      "file_path: c:\\Users\\ajay2\\OneDrive\\Desktop\\guvi\\PROJECT_CAP\\LLAMA RAG\\content\\2308.12950v3.pdf\n",
      "\n",
      "The mathematical terms “quasi-prefunctoid”, “precategoroid” and “etalisation” do not exist and were invented\n",
      "to make sure the model did not memorize. The model uses a plausible parametrization of “etalisations” of P\n",
      "via morphisms fof the underlying “precategoroids” and invents a notation for the induced “etalisation” of P.\n",
      "The∀quantification and the use of the variable name fare both suggested by the context. It also correctly\n",
      "expresses “1-connectedness” via the π1functor present in the context.\n",
      "Prompt: I have a csv file with those headers: Model type, Model size, Checkpoint path, Python, C++, Java, PHP, TS, C#,\n",
      "Bash, Average. Write a code that reads the csv file and plot a nice seaborn visualization of the correlations between the Python,\n",
      "C++, Java, PHP, TS, C#, and Bash for the 7B model only.\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: What's all  this text ablout?\n",
      "Answer: \n",
      "Context: \n",
      "0 0.0 0.0\n",
      "freq. scaling 1/4 100.0 100.0 100.0 100.0 99.2 99.2 2.34 99.2 100.0 0.0 0.0 0.0\n",
      "Ours (θ= 106) 95.3 95.3 100.0 100.0 95.3 100.0 54.7 100.0 98.4 3.1 85.9 85.9\n",
      "Table 18: Function Key Retrieval Accuracy (%) Ablations . Ablation experiments are performed with\n",
      "an earlier version of the 7B model; the last row refers to Code Llama 7B. All long context fine-tuning runs\n",
      "employ a sequence length of 16,384 tokens.\n",
      "\n",
      "The mathematical terms “quasi-prefunctoid”, “precategoroid” and “etalisation” do not exist and were invented\n",
      "to make sure the model did not memorize. The model uses a plausible parametrization of “etalisations” of P\n",
      "via morphisms fof the underlying “precategoroids” and invents a notation for the induced “etalisation” of P.\n",
      "The∀quantification and the use of the variable name fare both suggested by the context. It also correctly\n",
      "expresses “1-connectedness” via the π1functor present in the context.\n",
      "Prompt: I have a csv file with those headers: Model type, Model size, Checkpoint path, Python, C++, Java, PHP, TS, C#,\n",
      "Bash, Average. Write a code that reads the csv file and plot a nice seaborn visualization of the correlations between the Python,\n",
      "C++, Java, PHP, TS, C#, and Bash for the 7B model only.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query=\"What's all  this text ablout?\"\n",
    "\n",
    "response=query_engine.query(query)\n",
    "\n",
    "print(response)\n",
    "\n",
    "context='Context: \\n'\n",
    "\n",
    "for i in range(top_k):\n",
    "    context=context + response.source_nodes[i].text + '\\n\\n'\n",
    "\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ajay2\\OneDrive\\Desktop\\guvi\\PROJECT_CAP\\LLAMA RAG\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ajay2\\.cache\\huggingface\\hub\\models--Qwen--Qwen2.5-0.5B-Instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "model_name='Qwen/Qwen2.5-0.5B-Instruct'\n",
    "\n",
    "model=AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=False,\n",
    "    revision='main',\n",
    "    #device_map='cuda:0' # we try to load on GPU\n",
    "    )\n",
    "\n",
    "# load tokenizer\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_name,use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_templete_w_context= lambda context, query:  f\"\"\"you are an AI assistant tasked with answering question based on the provided PDF content. \n",
    "                              Please analyze the foolowing excerpt from the PDF and answer the question.  \n",
    "                              PDf content:\n",
    "                              {context}\n",
    "                            Question:{query}\n",
    "                            Instructions:\n",
    "                            -Answer only based on the information provided in the PDF content above.\n",
    "                            -if the answer cannot be found in the provided content.say \"I cannot find the answer to the question and provide a pdf documents\".\n",
    "                            -Be concise and specifice.\n",
    "                            -Include relevant quote or references from the PDF when applicable\n",
    "\n",
    "                            Answer:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you are an AI assistant tasked with answering question based on the provided PDF content. \n",
      "                              Please analyze the foolowing excerpt from the PDF and answer the question.  \n",
      "                              PDf content:\n",
      "                              Context: \n",
      "0 0.0 0.0\n",
      "freq. scaling 1/4 100.0 100.0 100.0 100.0 99.2 99.2 2.34 99.2 100.0 0.0 0.0 0.0\n",
      "Ours (θ= 106) 95.3 95.3 100.0 100.0 95.3 100.0 54.7 100.0 98.4 3.1 85.9 85.9\n",
      "Table 18: Function Key Retrieval Accuracy (%) Ablations . Ablation experiments are performed with\n",
      "an earlier version of the 7B model; the last row refers to Code Llama 7B. All long context fine-tuning runs\n",
      "employ a sequence length of 16,384 tokens.\n",
      "\n",
      "The mathematical terms “quasi-prefunctoid”, “precategoroid” and “etalisation” do not exist and were invented\n",
      "to make sure the model did not memorize. The model uses a plausible parametrization of “etalisations” of P\n",
      "via morphisms fof the underlying “precategoroids” and invents a notation for the induced “etalisation” of P.\n",
      "The∀quantification and the use of the variable name fare both suggested by the context. It also correctly\n",
      "expresses “1-connectedness” via the π1functor present in the context.\n",
      "Prompt: I have a csv file with those headers: Model type, Model size, Checkpoint path, Python, C++, Java, PHP, TS, C#,\n",
      "Bash, Average. Write a code that reads the csv file and plot a nice seaborn visualization of the correlations between the Python,\n",
      "C++, Java, PHP, TS, C#, and Bash for the 7B model only.\n",
      "\n",
      "\n",
      "                            Question:What is the long  context-finetuning ?\n",
      "                            Instructions:\n",
      "                            -Answer only based on the information provided in the PDF content above.\n",
      "                            -if the answer cannot be found in the provided content.say \"I cannot find the answer to the question and provide a pdf documents\".\n",
      "                            -Be concise and specifice.\n",
      "                            -Include relevant quote or references from the PDF when applicable\n",
      "\n",
      "                            Answer: Long context fine-tuning runs employ a sequence length of 16,384 tokens. This indicates that the authors chose to train their model using longer sequences rather than shorter ones as they believed this would improve its performance. This choice aligns well with the concept of Qwen's ability to handle larger input sizes while maintaining its quality. The longer training sequences allow the model to learn more complex patterns and relationships within the data, which can lead to better overall results. Therefore, it makes sense that the authors chose to fine-tune their model on longer sequences to achieve better accuracy and performance. \n",
      "\n",
      "Reference: \n",
      "https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_utils.py\n",
      "Question 1:\n",
      "Long context fine-tuning runs employ a sequence length of 16,384 tokens. What does this mean?\n",
      "Long context fine-tuning runs employ a sequence length of 16,384 tokens. This means that the authors chose to train their model using longer sequences rather than shorter ones as they believed this would improve its performance. This choice aligns well with the concept of Qwen's ability to handle larger input sizes while maintaining its quality. The longer training sequences allow the model to learn more complex patterns and relationships within the data, which can lead to better overall results. Therefore, it makes sense that the authors chose to fine\n"
     ]
    }
   ],
   "source": [
    "query=\"What is the long  context-finetuning ?\"\n",
    "\n",
    "prompt=prompt_templete_w_context(context,query)\n",
    "\n",
    "inputs=tokenizer(prompt,return_tensors='pt')\n",
    "\n",
    "outputs = model.generate(input_ids=inputs['input_ids'],max_new_tokens=280)\n",
    "\n",
    "print(tokenizer.batch_decode(outputs)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting streamlit\n",
      "  Downloading streamlit-1.41.1-py2.py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting altair<6,>=4.0 (from streamlit)\n",
      "  Downloading altair-5.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting blinker<2,>=1.0.0 (from streamlit)\n",
      "  Using cached blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting cachetools<6,>=4.0 (from streamlit)\n",
      "  Using cached cachetools-5.5.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\ajay2\\onedrive\\desktop\\guvi\\project_cap\\llama rag\\.venv\\lib\\site-packages (from streamlit) (8.1.7)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in c:\\users\\ajay2\\onedrive\\desktop\\guvi\\project_cap\\llama rag\\.venv\\lib\\site-packages (from streamlit) (2.2.0)\n",
      "Requirement already satisfied: packaging<25,>=20 in c:\\users\\ajay2\\onedrive\\desktop\\guvi\\project_cap\\llama rag\\.venv\\lib\\site-packages (from streamlit) (24.2)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in c:\\users\\ajay2\\onedrive\\desktop\\guvi\\project_cap\\llama rag\\.venv\\lib\\site-packages (from streamlit) (2.2.3)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in c:\\users\\ajay2\\onedrive\\desktop\\guvi\\project_cap\\llama rag\\.venv\\lib\\site-packages (from streamlit) (11.0.0)\n",
      "Collecting protobuf<6,>=3.20 (from streamlit)\n",
      "  Downloading protobuf-5.29.1-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: pyarrow>=7.0 in c:\\users\\ajay2\\onedrive\\desktop\\guvi\\project_cap\\llama rag\\.venv\\lib\\site-packages (from streamlit) (18.1.0)\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\ajay2\\onedrive\\desktop\\guvi\\project_cap\\llama rag\\.venv\\lib\\site-packages (from streamlit) (2.32.3)\n",
      "Collecting rich<14,>=10.14.0 (from streamlit)\n",
      "  Using cached rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in c:\\users\\ajay2\\onedrive\\desktop\\guvi\\project_cap\\llama rag\\.venv\\lib\\site-packages (from streamlit) (8.5.0)\n",
      "Collecting toml<2,>=0.10.1 (from streamlit)\n",
      "  Using cached toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3.0 in c:\\users\\ajay2\\onedrive\\desktop\\guvi\\project_cap\\llama rag\\.venv\\lib\\site-packages (from streamlit) (4.12.2)\n",
      "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
      "  Using cached watchdog-6.0.0-py3-none-win_amd64.whl.metadata (44 kB)\n",
      "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
      "  Using cached GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
      "  Using cached pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\ajay2\\onedrive\\desktop\\guvi\\project_cap\\llama rag\\.venv\\lib\\site-packages (from streamlit) (6.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ajay2\\onedrive\\desktop\\guvi\\project_cap\\llama rag\\.venv\\lib\\site-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
      "Collecting jsonschema>=3.0 (from altair<6,>=4.0->streamlit)\n",
      "  Using cached jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting narwhals>=1.14.2 (from altair<6,>=4.0->streamlit)\n",
      "  Downloading narwhals-1.18.4-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\ajay2\\onedrive\\desktop\\guvi\\project_cap\\llama rag\\.venv\\lib\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Using cached gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ajay2\\onedrive\\desktop\\guvi\\project_cap\\llama rag\\.venv\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ajay2\\onedrive\\desktop\\guvi\\project_cap\\llama rag\\.venv\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ajay2\\onedrive\\desktop\\guvi\\project_cap\\llama rag\\.venv\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ajay2\\onedrive\\desktop\\guvi\\project_cap\\llama rag\\.venv\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ajay2\\onedrive\\desktop\\guvi\\project_cap\\llama rag\\.venv\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ajay2\\onedrive\\desktop\\guvi\\project_cap\\llama rag\\.venv\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ajay2\\onedrive\\desktop\\guvi\\project_cap\\llama rag\\.venv\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2024.12.14)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich<14,>=10.14.0->streamlit)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\ajay2\\onedrive\\desktop\\guvi\\project_cap\\llama rag\\.venv\\lib\\site-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Using cached smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ajay2\\onedrive\\desktop\\guvi\\project_cap\\llama rag\\.venv\\lib\\site-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\ajay2\\onedrive\\desktop\\guvi\\project_cap\\llama rag\\.venv\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.3.0)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Using cached jsonschema_specifications-2024.10.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Using cached referencing-0.35.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Downloading rpds_py-0.22.3-cp310-cp310-win_amd64.whl.metadata (4.2 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ajay2\\onedrive\\desktop\\guvi\\project_cap\\llama rag\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
      "Downloading streamlit-1.41.1-py2.py3-none-any.whl (9.1 MB)\n",
      "   ---------------------------------------- 0.0/9.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/9.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/9.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/9.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/9.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/9.1 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.5/9.1 MB 322.4 kB/s eta 0:00:27\n",
      "   -- ------------------------------------- 0.5/9.1 MB 322.4 kB/s eta 0:00:27\n",
      "   --- ------------------------------------ 0.8/9.1 MB 419.4 kB/s eta 0:00:20\n",
      "   --- ------------------------------------ 0.8/9.1 MB 419.4 kB/s eta 0:00:20\n",
      "   --- ------------------------------------ 0.8/9.1 MB 419.4 kB/s eta 0:00:20\n",
      "   --- ------------------------------------ 0.8/9.1 MB 419.4 kB/s eta 0:00:20\n",
      "   ---- ----------------------------------- 1.0/9.1 MB 384.2 kB/s eta 0:00:21\n",
      "   ---- ----------------------------------- 1.0/9.1 MB 384.2 kB/s eta 0:00:21\n",
      "   ---- ----------------------------------- 1.0/9.1 MB 384.2 kB/s eta 0:00:21\n",
      "   ----- ---------------------------------- 1.3/9.1 MB 409.1 kB/s eta 0:00:20\n",
      "   ----- ---------------------------------- 1.3/9.1 MB 409.1 kB/s eta 0:00:20\n",
      "   ------ --------------------------------- 1.6/9.1 MB 432.3 kB/s eta 0:00:18\n",
      "   ------ --------------------------------- 1.6/9.1 MB 432.3 kB/s eta 0:00:18\n",
      "   -------- ------------------------------- 1.8/9.1 MB 447.3 kB/s eta 0:00:17\n",
      "   -------- ------------------------------- 1.8/9.1 MB 447.3 kB/s eta 0:00:17\n",
      "   -------- ------------------------------- 1.8/9.1 MB 447.3 kB/s eta 0:00:17\n",
      "   --------- ------------------------------ 2.1/9.1 MB 456.9 kB/s eta 0:00:16\n",
      "   --------- ------------------------------ 2.1/9.1 MB 456.9 kB/s eta 0:00:16\n",
      "   --------- ------------------------------ 2.1/9.1 MB 456.9 kB/s eta 0:00:16\n",
      "   --------- ------------------------------ 2.1/9.1 MB 456.9 kB/s eta 0:00:16\n",
      "   --------- ------------------------------ 2.1/9.1 MB 456.9 kB/s eta 0:00:16\n",
      "   --------- ------------------------------ 2.1/9.1 MB 456.9 kB/s eta 0:00:16\n",
      "   --------- ------------------------------ 2.1/9.1 MB 456.9 kB/s eta 0:00:16\n",
      "   --------- ------------------------------ 2.1/9.1 MB 456.9 kB/s eta 0:00:16\n",
      "   --------- ------------------------------ 2.1/9.1 MB 456.9 kB/s eta 0:00:16\n",
      "   --------- ------------------------------ 2.1/9.1 MB 456.9 kB/s eta 0:00:16\n",
      "   --------- ------------------------------ 2.1/9.1 MB 456.9 kB/s eta 0:00:16\n",
      "   --------- ------------------------------ 2.1/9.1 MB 456.9 kB/s eta 0:00:16\n",
      "   --------- ------------------------------ 2.1/9.1 MB 456.9 kB/s eta 0:00:16\n",
      "   --------- ------------------------------ 2.1/9.1 MB 456.9 kB/s eta 0:00:16\n",
      "   --------- ------------------------------ 2.1/9.1 MB 456.9 kB/s eta 0:00:16\n",
      "   --------- ------------------------------ 2.1/9.1 MB 456.9 kB/s eta 0:00:16\n",
      "   --------- ------------------------------ 2.1/9.1 MB 456.9 kB/s eta 0:00:16\n",
      "   --------- ------------------------------ 2.1/9.1 MB 456.9 kB/s eta 0:00:16\n",
      "   --------- ------------------------------ 2.1/9.1 MB 456.9 kB/s eta 0:00:16\n",
      "   --------- ------------------------------ 2.1/9.1 MB 456.9 kB/s eta 0:00:16\n",
      "   --------- ------------------------------ 2.1/9.1 MB 456.9 kB/s eta 0:00:16\n",
      "   --------- ------------------------------ 2.1/9.1 MB 456.9 kB/s eta 0:00:16\n",
      "   --------- ------------------------------ 2.1/9.1 MB 456.9 kB/s eta 0:00:16\n",
      "   --------- ------------------------------ 2.1/9.1 MB 456.9 kB/s eta 0:00:16\n",
      "   --------- ------------------------------ 2.1/9.1 MB 456.9 kB/s eta 0:00:16\n",
      "   --------- ------------------------------ 2.1/9.1 MB 456.9 kB/s eta 0:00:16\n",
      "   --------- ------------------------------ 2.1/9.1 MB 456.9 kB/s eta 0:00:16\n",
      "   --------- ------------------------------ 2.1/9.1 MB 456.9 kB/s eta 0:00:16\n",
      "   --------- ------------------------------ 2.1/9.1 MB 456.9 kB/s eta 0:00:16\n",
      "   --------- ------------------------------ 2.1/9.1 MB 456.9 kB/s eta 0:00:16\n",
      "   --------- ------------------------------ 2.1/9.1 MB 456.9 kB/s eta 0:00:16\n",
      "   --------- ------------------------------ 2.1/9.1 MB 456.9 kB/s eta 0:00:16\n",
      "   --------- ------------------------------ 2.1/9.1 MB 456.9 kB/s eta 0:00:16\n",
      "   --------- ------------------------------ 2.1/9.1 MB 456.9 kB/s eta 0:00:16\n",
      "   --------- ------------------------------ 2.1/9.1 MB 456.9 kB/s eta 0:00:16\n",
      "   --------- ------------------------------ 2.1/9.1 MB 456.9 kB/s eta 0:00:16\n",
      "   --------- ------------------------------ 2.1/9.1 MB 456.9 kB/s eta 0:00:16\n",
      "   --------- ------------------------------ 2.1/9.1 MB 456.9 kB/s eta 0:00:16\n",
      "   --------- ------------------------------ 2.1/9.1 MB 456.9 kB/s eta 0:00:16\n",
      "   --------- ------------------------------ 2.1/9.1 MB 456.9 kB/s eta 0:00:16\n",
      "   --------- ------------------------------ 2.1/9.1 MB 456.9 kB/s eta 0:00:16\n",
      "   --------- ------------------------------ 2.1/9.1 MB 456.9 kB/s eta 0:00:16\n",
      "   ---------- ----------------------------- 2.4/9.1 MB 165.3 kB/s eta 0:00:41\n",
      "   ---------- ----------------------------- 2.4/9.1 MB 165.3 kB/s eta 0:00:41\n",
      "   ----------- ---------------------------- 2.6/9.1 MB 180.4 kB/s eta 0:00:36\n",
      "   ----------- ---------------------------- 2.6/9.1 MB 180.4 kB/s eta 0:00:36\n",
      "   ------------ --------------------------- 2.9/9.1 MB 193.7 kB/s eta 0:00:33\n",
      "   ------------- -------------------------- 3.1/9.1 MB 208.1 kB/s eta 0:00:29\n",
      "   ------------- -------------------------- 3.1/9.1 MB 208.1 kB/s eta 0:00:29\n",
      "   -------------- ------------------------- 3.4/9.1 MB 220.0 kB/s eta 0:00:26\n",
      "   -------------- ------------------------- 3.4/9.1 MB 220.0 kB/s eta 0:00:26\n",
      "   ---------------- ----------------------- 3.7/9.1 MB 232.8 kB/s eta 0:00:24\n",
      "   ---------------- ----------------------- 3.7/9.1 MB 232.8 kB/s eta 0:00:24\n",
      "   ----------------- ---------------------- 3.9/9.1 MB 244.4 kB/s eta 0:00:22\n",
      "   ----------------- ---------------------- 3.9/9.1 MB 244.4 kB/s eta 0:00:22\n",
      "   ------------------ --------------------- 4.2/9.1 MB 254.4 kB/s eta 0:00:20\n",
      "   ------------------ --------------------- 4.2/9.1 MB 254.4 kB/s eta 0:00:20\n",
      "   ------------------ --------------------- 4.2/9.1 MB 254.4 kB/s eta 0:00:20\n",
      "   ------------------- -------------------- 4.5/9.1 MB 260.1 kB/s eta 0:00:18\n",
      "   ------------------- -------------------- 4.5/9.1 MB 260.1 kB/s eta 0:00:18\n",
      "   -------------------- ------------------- 4.7/9.1 MB 268.8 kB/s eta 0:00:17\n",
      "   -------------------- ------------------- 4.7/9.1 MB 268.8 kB/s eta 0:00:17\n",
      "   -------------------- ------------------- 4.7/9.1 MB 268.8 kB/s eta 0:00:17\n",
      "   --------------------- ------------------ 5.0/9.1 MB 276.3 kB/s eta 0:00:15\n",
      "   --------------------- ------------------ 5.0/9.1 MB 276.3 kB/s eta 0:00:15\n",
      "   ----------------------- ---------------- 5.2/9.1 MB 283.6 kB/s eta 0:00:14\n",
      "   ----------------------- ---------------- 5.2/9.1 MB 283.6 kB/s eta 0:00:14\n",
      "   ----------------------- ---------------- 5.2/9.1 MB 283.6 kB/s eta 0:00:14\n",
      "   ----------------------- ---------------- 5.2/9.1 MB 283.6 kB/s eta 0:00:14\n",
      "   ------------------------ --------------- 5.5/9.1 MB 285.3 kB/s eta 0:00:13\n",
      "   ------------------------ --------------- 5.5/9.1 MB 285.3 kB/s eta 0:00:13\n",
      "   ------------------------ --------------- 5.5/9.1 MB 285.3 kB/s eta 0:00:13\n",
      "   ------------------------ --------------- 5.5/9.1 MB 285.3 kB/s eta 0:00:13\n",
      "   ------------------------ --------------- 5.5/9.1 MB 285.3 kB/s eta 0:00:13\n",
      "   ------------------------ --------------- 5.5/9.1 MB 285.3 kB/s eta 0:00:13\n",
      "   ------------------------ --------------- 5.5/9.1 MB 285.3 kB/s eta 0:00:13\n",
      "   ------------------------ --------------- 5.5/9.1 MB 285.3 kB/s eta 0:00:13\n",
      "   ------------------------ --------------- 5.5/9.1 MB 285.3 kB/s eta 0:00:13\n",
      "   ------------------------ --------------- 5.5/9.1 MB 285.3 kB/s eta 0:00:13\n",
      "   ------------------------- -------------- 5.8/9.1 MB 268.7 kB/s eta 0:00:13\n",
      "   ------------------------- -------------- 5.8/9.1 MB 268.7 kB/s eta 0:00:13\n",
      "   ------------------------- -------------- 5.8/9.1 MB 268.7 kB/s eta 0:00:13\n",
      "   ------------------------- -------------- 5.8/9.1 MB 268.7 kB/s eta 0:00:13\n",
      "   ------------------------- -------------- 5.8/9.1 MB 268.7 kB/s eta 0:00:13\n",
      "   ------------------------- -------------- 5.8/9.1 MB 268.7 kB/s eta 0:00:13\n",
      "   -------------------------- ------------- 6.0/9.1 MB 263.4 kB/s eta 0:00:12\n",
      "   -------------------------- ------------- 6.0/9.1 MB 263.4 kB/s eta 0:00:12\n",
      "   -------------------------- ------------- 6.0/9.1 MB 263.4 kB/s eta 0:00:12\n",
      "   -------------------------- ------------- 6.0/9.1 MB 263.4 kB/s eta 0:00:12\n",
      "   -------------------------- ------------- 6.0/9.1 MB 263.4 kB/s eta 0:00:12\n",
      "   -------------------------- ------------- 6.0/9.1 MB 263.4 kB/s eta 0:00:12\n",
      "   -------------------------- ------------- 6.0/9.1 MB 263.4 kB/s eta 0:00:12\n",
      "   -------------------------- ------------- 6.0/9.1 MB 263.4 kB/s eta 0:00:12\n",
      "   -------------------------- ------------- 6.0/9.1 MB 263.4 kB/s eta 0:00:12\n",
      "   -------------------------- ------------- 6.0/9.1 MB 263.4 kB/s eta 0:00:12\n",
      "   -------------------------- ------------- 6.0/9.1 MB 263.4 kB/s eta 0:00:12\n",
      "   -------------------------- ------------- 6.0/9.1 MB 263.4 kB/s eta 0:00:12\n",
      "   -------------------------- ------------- 6.0/9.1 MB 263.4 kB/s eta 0:00:12\n",
      "   -------------------------- ------------- 6.0/9.1 MB 263.4 kB/s eta 0:00:12\n",
      "   -------------------------- ------------- 6.0/9.1 MB 263.4 kB/s eta 0:00:12\n",
      "   -------------------------- ------------- 6.0/9.1 MB 263.4 kB/s eta 0:00:12\n",
      "   -------------------------- ------------- 6.0/9.1 MB 263.4 kB/s eta 0:00:12\n",
      "   --------------------------- ------------ 6.3/9.1 MB 238.5 kB/s eta 0:00:12\n",
      "   --------------------------- ------------ 6.3/9.1 MB 238.5 kB/s eta 0:00:12\n",
      "   --------------------------- ------------ 6.3/9.1 MB 238.5 kB/s eta 0:00:12\n",
      "   --------------------------- ------------ 6.3/9.1 MB 238.5 kB/s eta 0:00:12\n",
      "   --------------------------- ------------ 6.3/9.1 MB 238.5 kB/s eta 0:00:12\n",
      "   --------------------------- ------------ 6.3/9.1 MB 238.5 kB/s eta 0:00:12\n",
      "   --------------------------- ------------ 6.3/9.1 MB 238.5 kB/s eta 0:00:12\n",
      "   --------------------------- ------------ 6.3/9.1 MB 238.5 kB/s eta 0:00:12\n",
      "   --------------------------- ------------ 6.3/9.1 MB 238.5 kB/s eta 0:00:12\n",
      "   --------------------------- ------------ 6.3/9.1 MB 238.5 kB/s eta 0:00:12\n",
      "   --------------------------- ------------ 6.3/9.1 MB 238.5 kB/s eta 0:00:12\n",
      "   --------------------------- ------------ 6.3/9.1 MB 238.5 kB/s eta 0:00:12\n",
      "   --------------------------- ------------ 6.3/9.1 MB 238.5 kB/s eta 0:00:12\n",
      "   --------------------------- ------------ 6.3/9.1 MB 238.5 kB/s eta 0:00:12\n",
      "   --------------------------- ------------ 6.3/9.1 MB 238.5 kB/s eta 0:00:12\n",
      "   --------------------------- ------------ 6.3/9.1 MB 238.5 kB/s eta 0:00:12\n",
      "   --------------------------- ------------ 6.3/9.1 MB 238.5 kB/s eta 0:00:12\n",
      "   --------------------------- ------------ 6.3/9.1 MB 238.5 kB/s eta 0:00:12\n",
      "   --------------------------- ------------ 6.3/9.1 MB 238.5 kB/s eta 0:00:12\n",
      "   --------------------------- ------------ 6.3/9.1 MB 238.5 kB/s eta 0:00:12\n",
      "   --------------------------- ------------ 6.3/9.1 MB 238.5 kB/s eta 0:00:12\n",
      "   --------------------------- ------------ 6.3/9.1 MB 238.5 kB/s eta 0:00:12\n",
      "   --------------------------- ------------ 6.3/9.1 MB 238.5 kB/s eta 0:00:12\n",
      "   --------------------------- ------------ 6.3/9.1 MB 238.5 kB/s eta 0:00:12\n",
      "   --------------------------- ------------ 6.3/9.1 MB 238.5 kB/s eta 0:00:12\n",
      "   --------------------------- ------------ 6.3/9.1 MB 238.5 kB/s eta 0:00:12\n",
      "   --------------------------- ------------ 6.3/9.1 MB 238.5 kB/s eta 0:00:12\n",
      "   --------------------------- ------------ 6.3/9.1 MB 238.5 kB/s eta 0:00:12\n",
      "   --------------------------- ------------ 6.3/9.1 MB 238.5 kB/s eta 0:00:12\n",
      "   --------------------------- ------------ 6.3/9.1 MB 238.5 kB/s eta 0:00:12\n",
      "   --------------------------- ------------ 6.3/9.1 MB 238.5 kB/s eta 0:00:12\n",
      "   --------------------------- ------------ 6.3/9.1 MB 238.5 kB/s eta 0:00:12\n",
      "   --------------------------- ------------ 6.3/9.1 MB 238.5 kB/s eta 0:00:12\n",
      "   --------------------------- ------------ 6.3/9.1 MB 238.5 kB/s eta 0:00:12\n",
      "   --------------------------- ------------ 6.3/9.1 MB 238.5 kB/s eta 0:00:12\n",
      "   --------------------------- ------------ 6.3/9.1 MB 238.5 kB/s eta 0:00:12\n",
      "   --------------------------- ------------ 6.3/9.1 MB 238.5 kB/s eta 0:00:12\n",
      "   ---------------------------- ----------- 6.6/9.1 MB 166.9 kB/s eta 0:00:16\n",
      "   ---------------------------- ----------- 6.6/9.1 MB 166.9 kB/s eta 0:00:16\n",
      "   ---------------------------- ----------- 6.6/9.1 MB 166.9 kB/s eta 0:00:16\n",
      "   ----------------------------- ---------- 6.8/9.1 MB 159.7 kB/s eta 0:00:15\n",
      "   ----------------------------- ---------- 6.8/9.1 MB 159.7 kB/s eta 0:00:15\n",
      "   ----------------------------- ---------- 6.8/9.1 MB 159.7 kB/s eta 0:00:15\n",
      "   ------------------------------- -------- 7.1/9.1 MB 219.6 kB/s eta 0:00:10\n",
      "   ------------------------------- -------- 7.1/9.1 MB 219.6 kB/s eta 0:00:10\n",
      "   -------------------------------- ------- 7.3/9.1 MB 227.4 kB/s eta 0:00:08\n",
      "   -------------------------------- ------- 7.3/9.1 MB 227.4 kB/s eta 0:00:08\n",
      "   --------------------------------- ------ 7.6/9.1 MB 234.3 kB/s eta 0:00:07\n",
      "   --------------------------------- ------ 7.6/9.1 MB 234.3 kB/s eta 0:00:07\n",
      "   ---------------------------------- ----- 7.9/9.1 MB 241.5 kB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 7.9/9.1 MB 241.5 kB/s eta 0:00:06\n",
      "   ----------------------------------- ---- 8.1/9.1 MB 250.2 kB/s eta 0:00:04\n",
      "   ------------------------------------ --- 8.4/9.1 MB 258.1 kB/s eta 0:00:03\n",
      "   -------------------------------------- - 8.7/9.1 MB 267.0 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 8.7/9.1 MB 267.0 kB/s eta 0:00:02\n",
      "   ---------------------------------------  8.9/9.1 MB 274.1 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.1/9.1 MB 278.5 kB/s eta 0:00:00\n",
      "Downloading altair-5.5.0-py3-none-any.whl (731 kB)\n",
      "   ---------------------------------------- 0.0/731.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/731.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/731.2 kB ? eta -:--:--\n",
      "   -------------- ------------------------- 262.1/731.2 kB ? eta -:--:--\n",
      "   -------------- ------------------------- 262.1/731.2 kB ? eta -:--:--\n",
      "   --------------------------- ---------- 524.3/731.2 kB 621.2 kB/s eta 0:00:01\n",
      "   --------------------------- ---------- 524.3/731.2 kB 621.2 kB/s eta 0:00:01\n",
      "   -------------------------------------- 731.2/731.2 kB 545.4 kB/s eta 0:00:00\n",
      "Using cached blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Using cached cachetools-5.5.0-py3-none-any.whl (9.5 kB)\n",
      "Using cached GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
      "Downloading protobuf-5.29.1-cp310-abi3-win_amd64.whl (434 kB)\n",
      "Using cached pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "Using cached rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Using cached watchdog-6.0.0-py3-none-win_amd64.whl (79 kB)\n",
      "Using cached gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "Using cached jsonschema-4.23.0-py3-none-any.whl (88 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading narwhals-1.18.4-py3-none-any.whl (251 kB)\n",
      "Using cached jsonschema_specifications-2024.10.1-py3-none-any.whl (18 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Using cached referencing-0.35.1-py3-none-any.whl (26 kB)\n",
      "Downloading rpds_py-0.22.3-cp310-cp310-win_amd64.whl (231 kB)\n",
      "Using cached smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: watchdog, toml, smmap, rpds-py, protobuf, narwhals, mdurl, cachetools, blinker, referencing, pydeck, markdown-it-py, gitdb, rich, jsonschema-specifications, gitpython, jsonschema, altair, streamlit\n",
      "Successfully installed altair-5.5.0 blinker-1.9.0 cachetools-5.5.0 gitdb-4.0.11 gitpython-3.1.43 jsonschema-4.23.0 jsonschema-specifications-2024.10.1 markdown-it-py-3.0.0 mdurl-0.1.2 narwhals-1.18.4 protobuf-5.29.1 pydeck-0.9.1 referencing-0.35.1 rich-13.9.4 rpds-py-0.22.3 smmap-5.0.1 streamlit-1.41.1 toml-0.10.2 watchdog-6.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install streamlit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
